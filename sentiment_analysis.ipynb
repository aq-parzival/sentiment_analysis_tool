{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyser:\n",
    "    def __init__(self):\n",
    "        # Initialize necessary NLTK downloads\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "            \n",
    "        try:\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "        except LookupError:\n",
    "            nltk.download('stopwords')\n",
    "            \n",
    "        try:\n",
    "            nltk.data.find('corpora/wordnet')\n",
    "        except LookupError:\n",
    "            nltk.download('wordnet')\n",
    "            \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.vectorizer = None\n",
    "        self.model = None\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and preprocess text data\"\"\"\n",
    "        # Handle NaN values\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "            \n",
    "        # Convert to string if needed\n",
    "        text = str(text)\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords and lemmatize\n",
    "        processed_tokens = [self.lemmatizer.lemmatize(word) for word in tokens if word not in self.stop_words and len(word) > 2]\n",
    "        \n",
    "        # Join tokens back into a string\n",
    "        return ' '.join(processed_tokens)\n",
    "    \n",
    "    def prepare_kaggle_data(self, train_path, test_path=None):\n",
    "        \"\"\"Load and prepare data from Kaggle dataset\"\"\"\n",
    "        # Load training data\n",
    "        print(\"Loading training dataset...\")\n",
    "        try:\n",
    "            train_data = pd.read_csv(train_path)\n",
    "            \n",
    "            # Check if required columns exist\n",
    "            if 'text' not in train_data.columns or 'sentiment' not in train_data.columns:\n",
    "                raise ValueError(\"Dataset must contain 'text' and 'sentiment' columns\")\n",
    "            \n",
    "            # Preprocess text\n",
    "            print(\"Preprocessing training text data...\")\n",
    "            train_data['processed_text'] = train_data['text'].apply(self.preprocess_text)\n",
    "            \n",
    "            # Encode sentiment\n",
    "            print(\"Encoding sentiment labels...\")\n",
    "            sentiment_mapping = {\n",
    "                'positive': 2, \n",
    "                'neutral': 1, \n",
    "                'negative': 0\n",
    "            }\n",
    "            train_data['sentiment_encoded'] = train_data['sentiment'].map(sentiment_mapping)\n",
    "            \n",
    "            # Load test data if provided\n",
    "            if test_path:\n",
    "                print(\"Loading test dataset...\")\n",
    "                test_data = pd.read_csv(test_path)\n",
    "                \n",
    "                if 'text' not in test_data.columns or 'sentiment' not in test_data.columns:\n",
    "                    raise ValueError(\"Test dataset must contain 'text' and 'sentiment' columns\")\n",
    "                \n",
    "                print(\"Preprocessing test text data...\")\n",
    "                test_data['processed_text'] = test_data['text'].apply(self.preprocess_text)\n",
    "                test_data['sentiment_encoded'] = test_data['sentiment'].map(sentiment_mapping)\n",
    "                \n",
    "                # Split training data\n",
    "                print(\"Splitting data into train and validation sets...\")\n",
    "                X_train, X_val, y_train, y_val = train_test_split(\n",
    "                    train_data['processed_text'], \n",
    "                    train_data['sentiment_encoded'],\n",
    "                    test_size=0.1,\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                # Prepare test data\n",
    "                X_test = test_data['processed_text']\n",
    "                y_test = test_data['sentiment_encoded']\n",
    "                \n",
    "                return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "                \n",
    "            else:\n",
    "                # Split data into train, validation, and test sets\n",
    "                print(\"Splitting data into train, validation, and test sets...\")\n",
    "                X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "                    train_data['processed_text'], \n",
    "                    train_data['sentiment_encoded'],\n",
    "                    test_size=0.3,\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                X_val, X_test, y_val, y_test = train_test_split(\n",
    "                    X_temp, \n",
    "                    y_temp,\n",
    "                    test_size=0.5,\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing data: {e}\")\n",
    "            return None, None, None, None, None, None\n",
    "            \n",
    "    def train_model(self, X_train, y_train):\n",
    "        \"\"\"Train sentiment analysis model\"\"\"\n",
    "        try:\n",
    "            # Create TF-IDF vectorizer\n",
    "            print(\"Creating TF-IDF features...\")\n",
    "            self.vectorizer = TfidfVectorizer(max_features=5000)\n",
    "            X_train_tfidf = self.vectorizer.fit_transform(X_train)\n",
    "            \n",
    "            # Train logistic regression model\n",
    "            print(\"Training logistic regression model...\")\n",
    "            self.model = LogisticRegression(C=1, max_iter=1000)\n",
    "            self.model.fit(X_train_tfidf, y_train)\n",
    "            \n",
    "            print(\"Model training complete!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training model: {e}\")\n",
    "            return False\n",
    "            \n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        try:\n",
    "            # Transform test data\n",
    "            X_test_tfidf = self.vectorizer.transform(X_test)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = self.model.predict(X_test_tfidf)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            report = classification_report(y_test, y_pred)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(report)\n",
    "            \n",
    "            # Plot confusion matrix\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            classes = ['Negative', 'Neutral', 'Positive']\n",
    "            tick_marks = np.arange(len(classes))\n",
    "            plt.xticks(tick_marks, classes, rotation=45)\n",
    "            plt.yticks(tick_marks, classes)\n",
    "            \n",
    "            # Add text annotations to confusion matrix\n",
    "            thresh = cm.max() / 2\n",
    "            for i in range(cm.shape[0]):\n",
    "                for j in range(cm.shape[1]):\n",
    "                    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                            horizontalalignment=\"center\",\n",
    "                            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "            \n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            return accuracy, report\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating model: {e}\")\n",
    "            return None, None\n",
    "            \n",
    "    def predict_sentiment(self, text):\n",
    "        \"\"\"Predict sentiment for new text\"\"\"\n",
    "        if self.model is None or self.vectorizer is None:\n",
    "            print(\"Model not trained. Please train the model first.\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Preprocess the text\n",
    "            processed_text = self.preprocess_text(text)\n",
    "            \n",
    "            # Transform using vectorizer\n",
    "            text_tfidf = self.vectorizer.transform([processed_text])\n",
    "            \n",
    "            # Predict\n",
    "            prediction = self.model.predict(text_tfidf)[0]\n",
    "            probabilities = self.model.predict_proba(text_tfidf)[0]\n",
    "            \n",
    "            # Map numeric prediction to sentiment\n",
    "            sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "            sentiment = sentiment_map[prediction]\n",
    "            \n",
    "            # Create result dictionary\n",
    "            result = {\n",
    "                'text': text,\n",
    "                'sentiment': sentiment,\n",
    "                'confidence': {\n",
    "                    'negative': round(probabilities[0], 4),\n",
    "                    'neutral': round(probabilities[1], 4) if len(probabilities) > 1 else 0,\n",
    "                    'positive': round(probabilities[2], 4) if len(probabilities) > 2 else 0\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting sentiment: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def save_model(self, model_path='sentiment_model.pkl', vectorizer_path='vectorizer.pkl'):\n",
    "        \"\"\"Save trained model and vectorizer to files\"\"\"\n",
    "        if self.model is None or self.vectorizer is None:\n",
    "            print(\"Model not trained. Nothing to save.\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(self.model, f)\n",
    "                \n",
    "            with open(vectorizer_path, 'wb') as f:\n",
    "                pickle.dump(self.vectorizer, f)\n",
    "                \n",
    "            print(f\"Model saved to {model_path} and vectorizer saved to {vectorizer_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "            return False\n",
    "            \n",
    "    def load_model(self, model_path='sentiment_model.pkl', vectorizer_path='vectorizer.pkl'):\n",
    "        \"\"\"Load trained model and vectorizer from files\"\"\"\n",
    "        try:\n",
    "            with open(model_path, 'rb') as f:\n",
    "                self.model = pickle.load(f)\n",
    "                \n",
    "            with open(vectorizer_path, 'rb') as f:\n",
    "                self.vectorizer = pickle.load(f)\n",
    "                \n",
    "            print(\"Model and vectorizer loaded successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
